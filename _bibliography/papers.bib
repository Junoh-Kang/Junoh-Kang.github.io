@InProceedings{kim2023openset,
    abbr      = {CVPR},
    author    = {Kim, Geeho and Kang, Junoh and Han, Bohyung},
    title     = {Open-Set Representation Learning Through Combinatorial Embedding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year      = {2023},
    pages     = {19744-19753},
    abstract  = {Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on both labeled and unlabeled examples, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. The representations given by the combinatorial embedding are made more robust by unsupervised pairwise relation learning. The proposed algorithm discovers novel concepts via a joint optimization for enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable performance gains by the proposed approach on public datasets for image retrieval and image categorization with novel class discovery.},
    pdf       = {kim2023openset.pdf},
    arxiv     = {2106.15278},
    selected  = {true},
}

@InProceedings{kang2024ogdm,
    abbr      = {CVPR},
    author    = {Kang*, Junoh and Choi*, Jinyoung and Choi, Sungik and Han, Bohyung},
    title     = {Observation-Guided Diffusion Probabilistic Models},
    booktitle = {CVPR},
    year      = {2024},
    pages     = {},
    abstract  = {We propose a novel diffusion-based image generation method called the observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling.
Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on a conditional discriminator on noise level, which employs a Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training scheme is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly the same inference procedure without incurring extra computational cost. We demonstrate the effectiveness of our training algorithm using diverse inference techniques on strong diffusion model baselines. Our implementation is available at https://github.com/Junoh-Kang/OGDM_edm.
},
    pdf       = {kang2024ogdm.pdf},
    code      = {https://github.com/Junoh-Kang/OGDM_edm},
    arxiv     = {2310.04041},
    selected  = {true},
}

@InProceedings{kim2024fifo,
    abbr      = {arXiv},
    author    = {Kim*, Jihwan and Kang*, Junoh and Choi, Jinyoung and Han, Bohyung},
    title     = {FIFO-Diffusion: Generating Infinite Videos from Text without Training},
    booktitle = {arXiv},
    year      = {2024},
    pages     = {},
    website = {https://jjihwan.github.io/projects/FIFO-Diffusion},
    selected  = {true},
}