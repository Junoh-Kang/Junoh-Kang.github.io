<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Diffusion Models in Two Perspectives | Junoh Kang </title> <meta name="author" content="Junoh Kang"> <meta name="description" content="A review of two papers, Denoising diffusion probabilistic models and Score-Based Generative Modeling through Stochastic Differential Equations. This post focuses on theoretical backgrounds of diffusion models, rather than implementations."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://junoh-kang.github.io/blog/2023/dl-Understanding-Diffusion-Models-in-Two-Perspectives/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Understanding Diffusion Models in Two Perspectives",
      "description": "A review of two papers, Denoising diffusion probabilistic models and Score-Based Generative Modeling through Stochastic Differential Equations. This post focuses on theoretical backgrounds of diffusion models, rather than implementations.",
      "published": "September 3, 2023",
      "authors": [
        {
          "author": "Junoh Kang",
          "authorURL": "https://junoh-kang.github.io/",
          "affiliations": [
            {
              "name": "Seoul National University",
              "url": ""
            }
          ]
        }
        
      ],
      "attachments": "20230903/presentation.pdf",
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding Diffusion Models in Two Perspectives</h1> <p>A review of two papers, Denoising diffusion probabilistic models and Score-Based Generative Modeling through Stochastic Differential Equations. This post focuses on theoretical backgrounds of diffusion models, rather than implementations.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> <div><a href="#maximizing-log-likelihood">Maximizing Log-Likelihood</a></div> <div><a href="#matching-marginal-distributions">Matching Marginal Distributions</a></div> </nav> </d-contents> <h2 id="overview">Overview</h2> <p><strong>DDPM</strong><d-cite key="ho2020denoising"></d-cite> and <strong>Score-Based Model</strong><d-cite key="song2021scorebased"></d-cite> introduce diffusion model as a new paradigm of generative models. Since the concepts of both papers are similar, one might regard <strong>Score-Based Model</strong><d-cite key="song2021scorebased"></d-cite> as only a continuous version of <strong>DDPM</strong><d-cite key="ho2020denoising"></d-cite>. Two papers have slight different views, even their loss functions and implementations are similar.</p> <p>This post mainly explains how formulations and objectives of two papers are different, and how they are related even with the differences.</p> <h4 id="summary-of-the-post">Summary of the post</h4> <ol> <li>The objective of <strong>DDPM</strong><d-cite key="ho2020denoising"></d-cite> is to minimize the surrogate of the negative log-likelihood.</li> <li>The objective of <strong>Score-Based Model</strong><d-cite key="song2021scorebased"></d-cite> is to match marginal distributions of forward SDE and backward SDE/ODE.</li> <li>Even with the differences, both derivations require the score function, differential of the log of the probability density function. The score functions are parametrized by neural networks and both papaers have similar loss functions.</li> </ol> <hr> <h2 id="maximizing-log-likelihood">Maximizing Log-Likelihood</h2> <h3 id="forward-diffusion-process">Forward (Diffusion) Process</h3> <p>The forward process is a Markov chain that gradually adds Gaussian noise to the data for \(T\) steps with distributions defined as follows: \(\begin{align} &amp;\mathrm{x}_t \perp\mkern-9.5mu\perp \mathrm{x}_{0:t-1}, \\ &amp;q(\mathrm{x}_0) := \mathrm{P}_{data}(\mathrm{x}_0), \\ &amp;q(\mathrm{x}_t|\mathrm{x}_{t-1}) := \mathcal{N}(\mathrm{x}_t;\sqrt{1-\beta_t}\mathrm{x}_{t-1}, \beta_t \mathrm{I}), \end{align}\)</p> <p>where \(\{\beta_t\}_{t=1}^T\) are pre-defined constants.</p> <h3 id="backward-denoising-process">Backward (Denoising) Process</h3> <p>The backward process is a Markov chain that gradually denoises perturbed data and it is parametrized by neural networks. When \(\beta_t\ll 1\) the backward distribution can be approximated as \(\begin{align} q(\mathrm{x}_{t-1}|\mathrm{x}_{t}) \approx \mathcal{N}(\mathrm{x}_{t-1}; \cfrac{1}{ \sqrt{1-\beta_t}}(\mathrm{x}_{t} + \beta_t \nabla \log q (\mathrm{x}_t)), \beta_t \mathrm{I}). \end{align}\)</p> <details><summary><em>proof.</em></summary> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blog/post/20230903/proof-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blog/post/20230903/proof-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blog/post/20230903/proof-1400.webp"></source> <img src="/blog/post/20230903/proof.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </details> <p>It is reasonable to parametrize the denoising distribution as Gaussian as long as \(\{\beta_t\}_{t=1}^T\) are infinitesimal. Therefore the bacward process is defined as follows: \(\begin{align} &amp;\mathrm{x}_t \perp\mkern-9.5mu\perp \mathrm{x}_{t+1:T}, \\ &amp;p(\mathrm{x}_T) := \mathcal{N}(\mathrm{x}_T; \mathrm{0}, \mathrm{I}) \\ &amp;p(\mathrm{x}_{t-1}|\mathrm{x}_{t}) = \mathcal{N}(\mathrm{x}_{t-1}; \cfrac{1}{ \sqrt{1-\beta_t}}(\mathrm{x}_{t} + \beta_t s_\theta(\mathrm{x}_t,t)), \beta_t \mathrm{I}), \end{align}\)</p> <p>Note that we expect \(s_\theta(\mathrm{x}_t,t)\) to learn \(\nabla\log q(\mathrm{x}_t)\).</p> <h3 id="minimizing-surrogate-of-negative-log-likelihood">Minimizing Surrogate of Negative Log-Likelihood</h3> <p>The negative log-likelihood of data is \(\begin{align} \mathbb{E}_{\mathrm{x}_0 \sim q} \left[-\log p(\mathrm{x}_0)\right] &amp;\leq \mathbb{E}_{\mathrm{x}_0 \sim q} \mathbb{E}_{\mathrm{x}_{1:T|0} \sim q} \left[ \log \cfrac{q(\mathrm{x}_{1:T}|\mathrm{x}_{0})}{p(\mathrm{x}_{0:T})} \right]. \end{align}\)</p> <details><summary><em>proof.</em></summary> \[\begin{align*} -\log p(\mathrm{x}_0) &amp;= -\log \int p(\mathrm{x}_{0:T}) d\mathrm{x}_{1:T} \\ &amp;= -\log \int q(\mathrm{x}_{1:T}|\mathrm{x}_{0}) \cfrac{p(\mathrm{x}_{0:T})}{q(\mathrm{x}_{1:T}|\mathrm{x}_{0})} d\mathrm{x}_{1:T} \\ &amp;\leq -\int q(\mathrm{x}_{1:T}|\mathrm{x}_{0}) \log \cfrac{p(\mathrm{x}_{0:T})}{q(\mathrm{x}_{1:T}|\mathrm{x}_{0})} d\mathrm{x}_{1:T} ~~(\because \text{Jensen})\\ &amp;= \mathbb{E}_{\mathrm{x}_{1:T|0} \sim q} \left[ \log \cfrac{q(\mathrm{x}_{1:T}|\mathrm{x}_{0})}{p(\mathrm{x}_{0:T})} \right]. \end{align*}\] </details> <p>Using Markov properties, \(\begin{align} &amp;q(\mathrm{x}_{1:T} | \mathrm{x}_0) = q(\mathrm{x}_T | \mathrm{x}_0) \prod_{t=2}^T q(\mathrm{x}_{t-1} | \mathrm{x}_t, \mathrm{x}_0), \\ &amp;p(\mathrm{x}_{T:0}) = p(\mathrm{x}_T) \prod_{t=T}^{1} p(\mathrm{x}_{t-1}|\mathrm{x}_t). \end{align}\)</p> <details><summary><em>proof.</em></summary> \[\begin{align*} q(\mathrm{x}_{1:T} | \mathrm{x}_0) &amp;= \prod_{t=1}^{T} q(\mathrm{x}_{t}|\mathrm{x}_{0:t-1}) \\ &amp;= \prod_{t=1}^{T} q(\mathrm{x}_{t}|\mathrm{x}_{t-1}) \\ &amp;= q(\mathrm{x}_1|\mathrm{x}_0)\prod_{t=2}^{T} q(\mathrm{x}_{t}|\mathrm{x}_{t-1}, \mathrm{x}_{0}) \\ &amp;= q(\mathrm{x}_1|\mathrm{x}_0)\prod_{t=2}^{T} \frac{q(\mathrm{x}_{t},\mathrm{x}_{t-1}| \mathrm{x}_{0})}{q(\mathrm{x}_{t-1}| \mathrm{x}_{0})} \\ &amp;= q(\mathrm{x}_1|\mathrm{x}_0)\prod_{t=2}^{T} \frac{q(\mathrm{x}_{t}|\mathrm{x}_{0})q(\mathrm{x}_{t-1}| \mathrm{x}_{t},\mathrm{x}_{0})}{q(\mathrm{x}_{t-1}| \mathrm{x}_{0})} \\ &amp;= q(\mathrm{x}_T | \mathrm{x}_0) \prod_{t=2}^T q(\mathrm{x}_{t-1} | \mathrm{x}_t, \mathrm{x}_0), \\ p(\mathrm{x}_{T:0}) &amp;= p(\mathrm{x}_T) \prod_{t=T}^{1} p(\mathrm{x}_{t-1}|\mathrm{x}_{T:t})\\ &amp;= p(\mathrm{x}_T) \prod_{t=T}^{1} p(\mathrm{x}_{t-1}|\mathrm{x}_t). \end{align*}\] </details> <p>Therefore, the surrogate of negative log-likelihood becomes \(\begin{align} &amp;D_{KL}(q(\mathrm{x}_T|\mathrm{x}_0) || p(\mathrm{x}_T)) + \mathbb{E}_q\left[-\log p(\mathrm{x}_0|\mathrm{x}_1)\right] \nonumber \\ &amp;+ \sum_{t=2}^T D_{KL}(q(\mathrm{x}_{t-1} | \mathrm{x}_t, \mathrm{x}_0) || p(\mathrm{x}_{t-1}|\mathrm{x}_t)). \end{align}\)</p> <p>The surrogate of negative log-likelihood can be explictly expressed using \(\begin{align} &amp;p(\mathrm{x}_{t-1}|\mathrm{x}_{t}) = \mathcal{N}(\mathrm{x}_{t-1}; \cfrac{1}{ \sqrt{1-\beta_t}}(\mathrm{x}_{t} + \beta_t s_\theta(\mathrm{x}_t,t)), \beta_t \mathrm{I}), \\ &amp;q(\mathrm{x}_{t-1}|\mathrm{x}_{t}, \mathrm{x}_{0}) = \mathcal{N}(\mathrm{x}_{t-1}; \cfrac{1}{ \sqrt{1-\beta_t}}(\mathrm{x}_{t} + \beta_t \nabla \log q (\mathrm{x}_t|\mathrm{x}_{0})), \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t \mathrm{I}), \end{align}\)</p> <p>where \(\bar\alpha_t = \prod_{s=1}^t (1-\beta_s)\).</p> <p>Finally, the objective function becomes \(\begin{align} \sum_{t=1}^T \mathbb{E}_{\mathrm{x}_0}\mathbb{E}_{\mathrm{x}_{t}|\mathrm{x}_{0}} \left[ \lambda_t ||s_\theta(\mathrm{x}_t,t) - \nabla \log q(\mathrm{x}_t|\mathrm{x}_0)||_2^2 \right], \end{align}\)</p> <p>where \(\lambda_t\) are some constants.</p> <hr> <h2 id="matching-marginal-distributions">Matching Marginal Distributions</h2> <h3 id="forward-sde">Forward SDE</h3> <p>For pre-defined function \(f:\mathbb{R}^{h\times w \times 3}\times \mathbb{R} \rightarrow \mathbb{R}^{h\times w \times 3}\) and \(g:\mathbb{R} \rightarrow \mathbb{R}\), a forward SDE perturbs the data with Gaussian noise by \(\begin{align} d\mathrm{x}_t = f(\mathrm{x}_t,t)dt + g(t)d\mathrm{w}_t, ~~\text{and}~~ \mathrm{x}_0 \sim \mathrm{P}_{data}, \end{align}\)</p> <p>where \(\mathrm{w}_t\) is Brownian process.</p> <p>If \(\{\mathrm{x}_t\}_{t=0}^T\) is a solution of the forward SDE, it can be treated as a sample from the joint distribution \(\{p_t\}_{t=0}^T\). However, learning joint distribution is difficult and our interest is only \(\mathrm{x}_0\), not \(\{\mathrm{x}_t\}_{t=0}^T\). Therefore, it suffices to consider weakened objective, learning how marginal distributions evolve as \(t\) changes. The evolution of the marginal distributions is goverened by the <strong>Fokker-Plank equation</strong>: \(\begin{align} \partial_t p_t = - \nabla_x (f \cdot p_t ) + \frac{1}{2} \mathrm{tr}(g^T ~\nabla_x^2p_t~ g). \end{align}\)</p> <h3 id="backward-sdeode">Backward SDE/ODE</h3> <p>Following backward SDE and ODE are known to have the same marginal distributions: \(\begin{align} &amp;d\mathrm{x}_t = \left[ f(\mathrm{x}_t,t)dt - g^2(t) \nabla \log p_t(\mathrm{x}_t) \right]dt + g(t)d\bar{\mathrm{w}}_t , ~~\text{and}~~ \mathrm{x}_T \sim \mathcal{N}(\mathrm{0}, \mathrm{I}), \\ &amp;d\mathrm{x}_t = \left[ f(\mathrm{x}_t,t)dt - \frac{1}{2} g^2(t) \nabla \log p_t(\mathrm{x}_t) \right]dt , ~~\text{and}~~ \mathrm{x}_T \sim \mathcal{N}(\mathrm{0}, \mathrm{I}), \end{align}\)</p> <p>where \(\bar{\mathrm{w}}_t\) is the reverse-time Brownian motion. </p> <p>Since \(f(\cdot, \cdot)\) and \(g(\cdot)\) are known, the only unknown component in backward SDE/ODE is \(\nabla \log p_t (\cdot)\) which is also known as a score function. The score function is parametrized by neural network, \(s_\theta(\mathrm{x}_t,t)\).</p> <h3 id="learning-score-function">Learning Score Function</h3> <p>Since we parametrized the score function with the neural network, we can consider a loss function of \(\begin{align} \int_{0}^{T} \lambda_t \mathbb{E}_{\mathrm{x}_t} \left[ ||s_\theta(\mathrm{x}_t,t) - \nabla \log p_t(\mathrm{x}_t)||_2^2 \right] dt, \end{align}\)</p> <p>where \(\lambda_t\) are some constants. Note that \(\nabla\log p_t(\mathrm{x}_t)\) is intractable and with some tricks, the loss function changes into tractable form: \(\begin{align} \int_{0}^{T} \lambda_t \mathbb{E}_{\mathrm{x}_0}\mathbb{E}_{\mathrm{x}_{t}|\mathrm{x}_{0}} \left[ ||s_\theta(\mathrm{x}_t,t) - \nabla \log p_{t|0}(\mathrm{x}_t|\mathrm{x}_0)||_2^2 \right] dt. \end{align}\)</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/diffusion.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Junoh-Kang/Junoh-Kang.github.io","data-repo-id":"R_kgDOJ2HI5w","data-category":"General","data-category-id":"DIC_kwDOJ2HI584CZLhE","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Junoh Kang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: July 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>