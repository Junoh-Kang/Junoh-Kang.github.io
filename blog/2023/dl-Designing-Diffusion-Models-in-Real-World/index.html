<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Designing Diffusion Models in Real World | Junoh Kang </title> <meta name="author" content="Junoh Kang"> <meta name="description" content="A review of a paper, Elucidating the Design Space of Diffusion-Based Generative Models. This post focuses on the reasons of the engineering details in the paper."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://junoh-kang.github.io/blog/2023/dl-Designing-Diffusion-Models-in-Real-World/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Designing Diffusion Models in Real World",
      "description": "A review of a paper, Elucidating the Design Space of Diffusion-Based Generative Models. This post focuses on the reasons of the engineering details in the paper.",
      "published": "October 26, 2023",
      "authors": [
        {
          "author": "Junoh Kang",
          "authorURL": "https://junoh-kang.github.io/",
          "affiliations": [
            {
              "name": "Seoul National University",
              "url": ""
            }
          ]
        }
        
      ],
      "attachments": "20231026/presentation.pdf",
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Designing Diffusion Models in Real World</h1> <p>A review of a paper, Elucidating the Design Space of Diffusion-Based Generative Models. This post focuses on the reasons of the engineering details in the paper.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> <div><a href="#revisit-diffusion-models">Revisit Diffusion Models</a></div> <div><a href="#improvements-to-training">Improvements to Training</a></div> <div><a href="#improvements-to-deterministic-sampling">Improvements to Deterministic Sampling</a></div> <div><a href="#"></a></div> </nav> </d-contents> <h2 id="overview">Overview</h2> <p>In deep learning, practical implementations are just as important as theoretical supports. Especially when proposing new paradigms, such as GANs, diffusion models, and transformers, <em>etc.</em>, engineering skills are essential to bring the paradigms into the real world. Even if the suggested designs on diffusion models in <strong>Elucidating the Design Space of Diffusion-Based Generative Models (EDM)</strong><d-cite key="karras2022elucidating"></d-cite> are not optimal, the choices of the designs are theoretically or empirically supported. Learning these reasonings may help to bring your theory into the real world.</p> <hr> <h2 id="revisit-diffusion-models">Revisit Diffusion Models</h2> <h3 id="reformulate-diffusion-models">Reformulate diffusion models</h3> <p><strong>Score-Based Model</strong><d-cite key="song2021scorebased"></d-cite> defines forward SDE and marginal distribution is calculated from forward SDE. However when it comes to training, marginal distribution is more important than the SDE and therefore <strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> defines the marginal distribution first.</p> \[\begin{align} p_t(\mathrm{x}) = s(t)^{-d}p(\mathrm{x} /s(t);\sigma(t)), \end{align}\] <p>where \(p(\mathrm{x};\sigma) = \left[p_{\text{data}} * \mathcal{N}(\mathrm{0}, \sigma^2\mathrm{I})\right](\mathrm{x})\).</p> <p>Then, the corresponding probability flow ODE is</p> \[\begin{align} d\mathrm{x} = \left[\dot{s}(t)/s(t) - s(t)^2 \dot{\sigma}(t)\sigma(t) \nabla_\mathrm{x} \log p(\mathrm{x}_t/s(t);\sigma(t)) \right] dt. \label{edm:ode} \end{align}\] <h3 id="obstacles-in-diffusion-models">Obstacles in diffusion models</h3> <p>Generation by diffusion models can interpreted as solving ODE:</p> \[\begin{align} d\mathrm{x} = f(\mathrm{x}_t, s(t), \sigma(t))dt. \end{align}\] <ol> <li> <p>\(f(\mathrm{x}_t, s(t), \sigma(t))\) is not known and it is parametrized by a network \(f_\theta(\mathrm{x}_t, s(t), \sigma(t))\). The inaccurate approximation on the target causes degradation. <br> <strong>→ Better training!</strong></p> </li> <li> <p>The solution at \(t=0\) given boundary condition at \(t=T\) is <br> \(\begin{align} \mathrm{x}_0 = \mathrm{x}_T + \int_0^T f(\mathrm{x}_t, s(t), \sigma(t))dt. \end{align}\) The integral is numerically calculated, which causes truncation errors. <br> <strong>→ Reduce truncation errors, focus on important region!</strong></p> </li> </ol> <h3 id="design-space-of-diffusion-models">Design space of diffusion models</h3> <h4 id="components-regarding-training">Components regarding training</h4> <ul> <li>Parametrization and network preconditioning: \(c_\text{skip}(\sigma)\), \(c_\text{out}(\sigma)\), \(c_\text{in}(\sigma)\), \(c_\text{noise}(\sigma)\).</li> <li>Loss weighting: \(\lambda(t)\).</li> <li>Noise level distribution for training: \(\sigma \sim p_{\text{noise}}\).</li> <li>Augmentation</li> </ul> <h4 id="components-regarding-deterministic-sampling">Components regarding deterministic sampling</h4> <ul> <li>Truncation-error-reducing ODE: \(s(t)\), \(\sigma(t)\).</li> <li>Truncation-error-reducing algorithms: Higher-roder integrators</li> <li>Distributing truncation errors properly: Discretization \(\{t_i\}_0^N\)</li> </ul> <h4 id="components-regarding-stochastic-sampling">Components regarding stochastic sampling</h4> <ul> <li>Rate of replaced noises: \(\beta(t)\)</li> <li>Heuristics: \(S_{\text{tmin}}, S_{\text{tmax}}, S_{\text{noise}}, S_{\text{churn}}\).</li> </ul> <hr> <h2 id="improvements-to-training">Improvements to Training</h2> <p>For this section, this post assumes \(s(t)=1\).</p> <h3 id="parametrization-network-preconditioning-loss-weighting">Parametrization, network preconditioning, loss weighting</h3> <p>\(D(\mathrm{x}_t,\sigma)\) is a denoiser which minimizes \(\ell_2\)-norm with \(\mathrm{y}\): \(\begin{align} \mathbb{E}_{\mathrm{y} \sim p_{\text{data}}} \mathbb{E}||D(\mathrm{y} + \mathrm{n}) - \mathrm{y}||_2^2. \label{eq:loss} \end{align}\)</p> <p>Then, the relation between a score function and the ideal denoiser is \(\begin{align} \nabla_{\mathrm{x}} \log p(\mathrm{x};\sigma) = (D(\mathrm{x};\sigma) - \mathrm{x}) / \sigma^2. \end{align}\)</p> <p>Networks in many baselines predicts either \(D(\mathrm{x},\sigma)\) or \(\mathrm{n}\). However, <strong>Dynamic dual-output diffusion models</strong><d-cite key="benny2022dynamic"></d-cite> observes that predicting \(D(\mathrm{x},\sigma)\) is easier for high noise level, while predicting \(\mathrm{n}\) is easier for low noise level.</p> <table> <thead> <tr> <th style="text-align: center"><img src="/blog/post/20231026/benny.png" alt="" style="margin:auto; display:block;width:60%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Loss comparison between predicting the denoised output or the added noise.</td> </tr> </tbody> </table> <p>From the observation, <strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> designs the network to predict \(D(\mathrm{x};\sigma)\) or \(\mathrm{n}\), or something in between according to the noise level. \(\begin{align} D_\theta(\mathrm{x};\sigma) = c_{\text{skip}}(\sigma)\mathrm{x} + c_{\text{out}}(\sigma) F_\theta(c_{\text{in}}(\sigma)\mathrm{x}; c_{\text{noise}}(\sigma)), \end{align}\) where \(F_\theta\) is a neural network.</p> <p>Then the loss function (\ref{eq:loss}) is \(\begin{align} \mathbb{E}_{\sigma,\mathrm{y},\mathbf{b}}\left[ \underbrace{\lambda(\sigma)c_{\text{out}}(\sigma)^2}_{\text{effective weight}} ||\underbrace{F_\theta(c_{\text{in}}(\sigma)(\mathrm{y}+\mathbf{n};c_{\text{noise}}(\sigma)))}_{\text{network output}} - \underbrace{\frac{1}{c_{\text{out}}(\sigma)}(\mathrm{y} - c_{\text{skip}}(\sigma)(\mathrm{y}+\textbf{n}))}_{\text{effective training target}}||_2^2 \right]. \end{align}\)</p> <h4 id="1network-inputs-should-have-bounded-range">1.Network inputs should have bounded range</h4> <p>\(\begin{align} \Rightarrow &amp;\text{Var}_{\mathrm{y},\mathbf{n}}\left[c_{\text{in}}(\sigma)(\mathrm{y} + \mathbf{n})\right] = 1 \\ \Rightarrow &amp;c_{\text{in}}(\sigma) = 1 / \sqrt{\sigma^2 + \sigma_{\text{data}}^2}\\ \text{&amp; } &amp;c_{\text{noise}}(\sigma) = \log (\sigma)/4 \end{align}\)</p> <h4 id="2effective-training-target-should-have-bounded-range">2.Effective training target should have bounded range</h4> <p>\(\begin{align} &amp;\Rightarrow \text{Var}_{\mathrm{y},\mathbf{n}}\left[\frac{1}{c_{\text{out}}(\sigma)}(\mathrm{y} - c_{\text{skip}}(\sigma)(\mathrm{y}+\textbf{n}))\right] = 1 \\ &amp;\Rightarrow c_{\text{out}}(\sigma)^2 = (1-c_{\text{skip}}(\sigma))^2\sigma_{\text{data}}^2 + c_{\text{skip}}(\sigma)^2 \sigma^2 \end{align}\)</p> <h4 id="3errors-of-network-should-not-be-amplified">3.Errors of network should not be amplified</h4> <p>\(\begin{align} \Rightarrow~ &amp;c_{\text{skip}}(\sigma) = \underset{c_{\text{skip}}(\sigma)}{\text{argmin}} ~c_{\text{out}}(\sigma) \\ \Rightarrow~ &amp;\begin{cases} c_{\text{skip}}(\sigma) = \sigma_{\text{data}}^2 / (\sigma^2 + \sigma_{\text{data}}^2) \\ c_{\text{out}}(\sigma) = \sigma \cdot \sigma_{\text{data}} / \sqrt{\sigma^2 + \sigma_{\text{data}}^2} \end{cases} \end{align}\)</p> <h4 id="4-effecitve-weight-should-be-uniform">4. Effecitve weight should be uniform</h4> <p>\(\begin{align} \Rightarrow~ &amp;\lambda(\sigma)c_{\text{out}}(\sigma)^2 = 1 \\ \Rightarrow~ &amp; \lambda(\sigma) = (\sigma^2 + \sigma_{\text{data}}^2)/(\sigma \cdot \sigma_{\text{data}})^2 \end{align}\)</p> <p>Putting 1 ~ 4 together, the expected value of the loss at each noise level is 1. Moreover, the change of effective training target according to \(\sigma\) coincides to the observation of <strong>Dynamic dual-output diffusion models</strong><d-cite key="benny2022dynamic"></d-cite>.</p> <h3 id="noise-level-distribution-for-training">Noise level distribution for training</h3> <table> <thead> <tr> <th style="text-align: left"><img src="/blog/post/20231026/edm_f5a.png" alt="" style="margin:auto; display:block; width:90%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Observed loss per noise level. The shaded regions represent the standard deviation over 10k random samples. EDM’s proposed training sample density is shown by the dashed red curve.</td> </tr> </tbody> </table> <p>At low noise levels, seperating the small noise components is difficult and irrelevant, whereas at high noise levels, the correct answer approaches to dataset average; <strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> focuses on middle range noise levels for training: \(\sigma \sim \mathcal{N}(-1.2, 1.2)\).</p> <h3 id="augmentataion">Augmentataion</h3> <p><strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> follows the augmentation pipiline from the GAN literature<d-cite key="karras2020training"></d-cite>. <img src="/blog/post/20231026/edm_t6.png" alt="" style="margin:auto; display:block; width:90%; height:auto;"></p> <ol> <li>Each agmentation is enabled with \(A_{\text{prob}}\).</li> <li>Draw \(a_i\) from each enabled augmentation and construct transformation matrix.</li> <li>Pass data through \(2\times\) supersampled high-quality Wavelet filters.</li> <li>Construct a 9-dimensional conditioning input vector for non-leaking augmentation. This vector makes the network to perform auxiliarty tasks.</li> </ol> <hr> <h2 id="improvements-to-deterministic-sampling">Improvements to Deterministic Sampling</h2> <h3 id="higher-order-integrators">Higher-order integrators</h3> <p>For \(s(t)=1\) and \(\sigma(t)=t\), the ODE to solve is</p> \[\begin{align} d\mathrm{x}/dt = (\mathrm{x}_t - D(\mathrm{x}_t;t))/t := f(\mathrm{x}_t,t). \end{align}\] <h4 id="euler-method"><a href="https://en.wikipedia.org/wiki/Euler_method" rel="external nofollow noopener" target="_blank"><em>Euler method</em></a></h4> <p>Euler method approximates the integral by</p> \[\begin{align} \int_{t_{i}}^{t_{i-1}} f(\mathrm{x}_t,t) dt = (t_{i-1} - t_{i})f(\mathrm{x}_{t_i},t_i) + O(|t_{i-1}-t_{i}|^2). \end{align}\] <p>Therefore, the total truncation error is \(O(\max \lvert t_{i-1}-t_{i} \rvert)\). Let \(\hat{\mathrm{x}}_{t_{i-1}}\) is a solution obtained by Euler method.</p> <h4 id="heuns-method"><a href="https://en.wikipedia.org/wiki/Heun%27s_method" rel="external nofollow noopener" target="_blank"><em>Heun’s method</em></a></h4> <p>Then, Heun’s method approximates the integral by</p> \[\begin{align} \int_{t_{i}}^{t_{i-1}} f(\mathrm{x}_t,t) dt = (t_{i-1} - t_{i})(f(\mathrm{x}_{t_i},t_i)+ f(\hat{\mathrm{x}}_{t_{i-1}},t_{i-1}))/2+ O(|t_{i-1}-t_{i}|^3). \end{align}\] <p>Therefore, the total truncation error is \(O(\max{\lvert t_{i-1}-t_{i}|^2 \rvert})\). Huen’s method decreases truncation error at the cost of one additional evaluation of the network.</p> <h4 id="deterministic-sampling-algorithm-for-edm">Deterministic sampling algorithm for <strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> </h4> <p><img src="/blog/post/20231026/edm_a1.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></p> <h3 id="discretization">Discretization</h3> <p>As long as using numerical integrators with limited computational resources, <strong>truncation errors are inevitable</strong>. In terms of obtaining ODE trajectories accurately, it is important to minimize total truncation erros. Hoever, the interests of diffusion models at generation are only the <strong>solutions at low noise levels</strong>; it is reasonable to <strong>focus on low noise levels</strong>. EDM discretizes as</p> \[\begin{align} t_{N-i} = \sigma_{i&lt; N} = (\sigma_{\text{max}}^{1/\rho} + \frac{i}{N-1} (\sigma_{\text{min}}^{1/\rho} - \sigma_{\text{max}}^{1/\rho}))^\rho, \sigma_N = 0. \end{align}\] <p>Increasing \(\rho\) results dense discretizations at low noise levels.</p> <table> <thead> <tr> <th style="text-align: center"><img src="/blog/post/20231026/edm_f12.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">(a),(b) Local truncation error at different noise levels. (c) FID as a function of \(\rho\).</td> </tr> </tbody> </table> <p>\(\rho=3\) nearly equalizes the truncation error at each step as in (a), (b). On the other hand, \(\rho=7\) generates better samples as in (c). Proper value of \(\rho\) changes according to the tasks. <em>e.g.</em>, equalized truncation error will be better for solving ODE in both directions.</p> <h3 id="truncation-error-reducing-ode">Truncation-error-reducing ODE</h3> <p>Many integrators including Euler and Heun’s method have small truncation errors if \(f(\mathrm{x}_t,t)\) has <strong>small curvature</strong>, or is close to linear function. \(s(t)\) and \(\sigma(t)\) determine the shape of the ODE solution trajectories, which is closely related to linearity of the \(f(\cdot)\).</p> \[\int_{t_{i}}^{t_{i-1}} f(\mathrm{x}_t,t) dt \approx \begin{cases} (t_{i-1} - t_{i})f(\mathrm{x}_{t_i},t_i) &amp; \text{Euler method} \\ (t_{i-1} - t_{i}) (f(\mathrm{x}_{t_i},t_i)+ f(\hat{\mathrm{x}}_{t_{i-1}},t_{i-1}))/2 &amp; \text{Heun's method} \end{cases}\] <table> <thead> <tr> <th style="text-align: left"><img src="/blog/post/20231026/edm_f3.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">A sketch of ODE curvature in 1D where \(p_{\text{data}}\) is two Dirac peaks at \(\mathrm{x}= \pm 1\). Axis is chosen to show \(\sigma \in [0,25]\) and zoom in \(\sigma \in [0,1]\). (c) sketches the curvature when \(s(t)=1\) and \(\sigma(t)=t\). It has small curvature, while the tangent directs to the datapoints.</td> </tr> </tbody> </table> <h3 id="results-of-deterministic-sampling">Results of deterministic sampling</h3> <p><img src="/blog/post/20231026/edm_t2.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></p> <ul> <li>Config B changes basic hyperparameters such as batch size, learning rate, dropout, <em>etc</em>; it disable gradient clipping</li> <li>Config C improves the expressive power of the model.</li> <li>Configs D, E, and F are explained in the previous context.</li> </ul> <hr> <h2 id="stochastic-sampling">Stochastic Sampling</h2> <h3 id="sde-formulation">SDE formulation</h3> <p><strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> reformulates forward and backward SDE as a sum of the probability flow ODE and a varying-rate <a href="https://en.wikipedia.org/wiki/Langevin_dynamics" rel="external nofollow noopener" target="_blank"><em>Langevin diffusion</em></a> SDE: \(\begin{align} d\mathrm{x}_{\pm} = \underbrace{-\dot{\sigma}(t)\sigma(t) \nabla_\mathrm{x} \log p(\mathrm{x};\sigma(t)) dt}_{\text{probability flow ODE}} \pm \underbrace{\underbrace{\beta(t)\sigma(t)^2 \nabla_\mathrm{x} \log p(\mathrm{x};\sigma(t)) dt}_{\text{deterministic noise decay}} + \underbrace{\sqrt{2\beta(t)}\sigma(t) d\mathrm{w}_t}_{\text{noise injection}}}_{\text{Langevin diffusion SDE}} \end{align}\)</p> <h4 id="role-of-stochasticity">Role of stochasticity</h4> <p>In theory, ODE and SDE have the same marginal distributions. However in practice, stochasticity in sampling often enhances the sample quality. The authors attribute the beneficial role of stochasticity to the following steps:</p> <ol> <li>\(\mathrm{x}_t\) deviates from the ideal marginal distribution, due to the training and truncation errors.</li> <li>The <em>Langevin diffusion</em> drives the sample towards the ideal marginal distribution.</li> </ol> <h4 id="stochastic-sampling-algorithm-in-edm">Stochastic sampling algorithm in EDM</h4> <p><img src="/blog/post/20231026/edm_a2.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></p> <p>Stochastic sampling algorithm in <strong>EDM</strong><d-cite key="karras2022elucidating"></d-cite> is executed in two steps:</p> <ol> <li> <strong>Noise injection</strong>: integrate noise into samples according to \(\gamma_i \geq 0\).</li> <li> <strong>Noise decay with probability flow</strong>: solve the ODE from increased noise level to desired level.</li> </ol> <h3 id="algorithm-in-real-world">Algorithm in real world</h3> <table> <thead> <tr> <th style="text-align: left"><img src="/blog/post/20231026/edm_f13.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Observe the effect of <em>Langevin diffusion</em> in real world: there is gradual image degradation with the repeated addition and removal of noise. A random image is drawn from \(p(\mathrm{x};\sigma)\) and Algorithm 2 is run for a certain number of steps with \(\gamma_i=\sqrt{2}-1\).</td> </tr> </tbody> </table> <p><em>Langevin diffusion</em> is supposed to drive the sample towards the true data distribution, however…</p> <ul> <li>For low noise levels, images drift toward <strong>oversaturated colors</strong>.</li> <li>For high noise levels, images become abstract when \(s_{\text{noise}}=1\).</li> </ul> <p>Authors suspect that <strong>non-conservative vector field</strong> generated by parametrized denoiser <strong>violates the premises of Langevin diffusion</strong> since their analytical denoisers have not shown such degradation.</p> <p>\(\Rightarrow\) Fix flaws of \(D_\theta(\mathrm{x};\sigma)\) with heuristics!</p> <ul> <li> <p>For low noise levels, images drift toward <strong>oversaturated colors</strong>.<br> \(\Rightarrow\) Enable stochasticity within \(t_i \in [S_{\text{tmin}}, \underline{S_{\text{tmax}}}]\).</p> </li> <li> <p>For high noise levels, images become <strong>abstract</strong> when \(S_{\text{noise}}=1\). <br> \(\Rightarrow\) \(D_\theta(\cdot)\) removes too much noise because of <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean" rel="external nofollow noopener" target="_blank"><em>regression towards the mean</em></a>, which often happens when \(\ell_2\) trained.<br> \(\Rightarrow\) Inflate the standard deviation of newly added noise: \(S_{\text{noise}}&gt;1\).</p> </li> <li> <p>New noise never exceeds the noise already in the image. <br> \(\Rightarrow\) Clamp \(\gamma_i\).</p> </li> <li> <p>Controls the overal stochasticity by \(S_{\text{churn}}\).</p> </li> </ul> <h3 id="results-of-stochastic-sampling">Results of stochastic sampling</h3> <table> <thead> <tr> <th style="text-align: left"><img src="/blog/post/20231026/edm_f4.png" alt="" style="margin:auto; display:block; width:100%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Evaluation of stochastic samplers with ablations. Red line is deterministic sampler while purple line is optimal stochastic sampler.</td> </tr> </tbody> </table> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/diffusion.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Junoh-Kang/Junoh-Kang.github.io","data-repo-id":"R_kgDOJ2HI5w","data-category":"General","data-category-id":"DIC_kwDOJ2HI584CZLhE","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Junoh Kang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: June 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>