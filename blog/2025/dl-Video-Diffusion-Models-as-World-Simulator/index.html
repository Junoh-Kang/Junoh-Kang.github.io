<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Video Diffusion Models as World Simulators | Junoh Kang </title> <meta name="author" content="Junoh Kang"> <meta name="description" content="A review of Oasis: A Universe in a transformer, and Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://junoh-kang.github.io/blog/2025/dl-Video-Diffusion-Models-as-World-Simulator/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Video Diffusion Models as World Simulators",
      "description": "A review of Oasis: A Universe in a transformer, and Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
      "published": "January 17, 2025",
      "authors": [
        {
          "author": "Junoh Kang",
          "authorURL": "https://junoh-kang.github.io/",
          "affiliations": [
            {
              "name": "Seoul National University",
              "url": ""
            }
          ]
        }
        
      ],
      "attachments": "https://docs.google.com/presentation/d/1anuY_SaTFu5gXceB_qtCFLKER0aAlHsJq6CWN9KAGhE/edit?usp=sharing",
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Video Diffusion Models as World Simulators</h1> <p>A review of Oasis: A Universe in a transformer, and Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> <div><a href="#conventional-long-video-generation">Conventional Long Video Generation</a></div> <div><a href="#long-sequence-generation">Long Sequence Generation</a></div> <ul> <li><a href="#capability-of-long-sequence-generation">Capability of Long Sequence Generation</a></li> <li><a href="#preventing-error-accumulation">Preventing Error Accumulation</a></li> <li><a href="#long-term-context-preservation">Long Term Context Preservation</a></li> </ul> </nav> </d-contents> <h2 id="overview">Overview</h2> <p><strong>World simulators</strong> are explorable and interactive systems or models that can mimic real world. Advanced video generation models can function as world simulators, and to achieve it, they should have <strong>low latency</strong> for input actions, and capable of <strong>long sequence generation</strong>. Long sequence generation includes <strong><u>capability of long generation itself</u></strong>, <strong><u>preventing error accumulation</u></strong>, and <strong><u>long term context preservation</u></strong>. This post mainly focuses on how related project <strong>Oasis: A Universe in a transformer</strong><d-cite key="decart2024oasis"></d-cite> deals with <strong>long sequence generation</strong>.</p> <hr> <h2 id="conventional-long-video-generation-are-inappropriate-for-world-simulator">Conventional Long Video Generation are Inappropriate for World Simulator!</h2> <h3 id="video-diffusion-models-vdms">Video Diffusion Models (VDMs)</h3> <table> <thead> <tr> <th style="text-align: left"><img src="/blog/post/20250117/1.vdm.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Training and sampling of video diffusion models. Darker tokens has higher noise levels.</td> </tr> </tbody> </table> <p><strong><u>Videos are sequential data</u></strong>. However, video diffusion models are trained and inferenced to denoise tokens of same noise levels, interpreting each video clip as a single object. This section reviews approaches to generate long videos using aforementioned VDMs.</p> <h3 id="chunked-autoregressive-methods">Chunked Autoregressive Methods</h3> <table> <tbody> <tr> <td><img src="/blog/post/20250117/2.chunked.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <ul> <li>Small \(k\) (<em>e.g.</em> \(k=1\)) results in high latency for each action since \(f-k\) frames are output for each action. Also it tends to lose contexts.</li> <li>Large \(k\) (<em>e.g.</em> \(k=f-1\)) results in ineifficient training and inference since models learn only \(f-k\) tokens, while models calculate for \(f\) tokens.</li> <li>Chunked autoregressive methods suffers from <strong><u>quality degradation originated from error accumulation</u></strong>.</li> </ul> <table> <tbody> <tr> <td><img src="/blog/post/20250117/3.chunked_erroraccumulate.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <h3 id="hierarchical-methods-multi-stage-generation">Hierarchical Methods (Multi-stage Generation)</h3> <table> <tbody> <tr> <td><img src="/blog/post/20250117/4.hierarchy.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <ul> <li>It does not fit to interactive generation since the end of the video is already determined.</li> </ul> <p><strong>Conventional approaches are not appropriate for wolrd simulator!</strong></p> <table> <tbody> <tr> <td><img src="/blog/post/20250117/5.convention.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <hr> <h2 id="long-sequence-generation-in-oasis">Long Sequence Generation in Oasis</h2> <h3 id="capability-of-long-sequence-generation">Capability of Long Sequence Generation</h3> <p><strong>Oasis</strong><d-cite key="decart2024oasis"></d-cite> follows <strong>Diffusion Forcing</strong><d-cite key="chen2024diffusionforcing"></d-cite> to train models for long video generation. <strong>Diffusion Forcing</strong> inherits advantages of Teacher Forcing and Diffusion Models: <strong><u>flexible time horizon</u></strong> from Teacher Forcing, <strong><u>guidance at sampling</u></strong> from Diffusion Models.</p> <p><strong>Diffusion Forcing</strong> trains models to denoise <strong><u>tokens with independent noise levels</u></strong>, and sampling noise schedules are carefully chosen depending on the purpose. The training offers cheaper training than next-token prediction in video domain, and the complexity added by independent noise level is not excessive since the complexity is only in temporal dimension.</p> <table> <thead> <tr> <th style="text-align: center"><img src="/blog/post/20250117/6.df_train.png" alt="" style="margin:auto; display:block;width:50%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Training in Diffusion Forcing</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: center"><img src="/blog/post/20250117/7.df_sample.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Sampling in Diffusion Forcing</td> </tr> </tbody> </table> <h3 id="preventing-error-accumulation">Preventing Error Accumulation</h3> <h4 id="the-reason-of-error-accumulation">The reason of Error Accumulation</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/8.vanilla.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p><strong>Oasis</strong><d-cite key="decart2024oasis"></d-cite> and <strong>Diffusion Forcing</strong><d-cite key="chen2024diffusionforcing"></d-cite> hypothesize that the error accumulation stems from the model erroneously treating generated noisy frames as grount truth (GT), despite their inherent inaccuracies. They interpret <strong>input noise levels to the models as inversely proportional to the confidence</strong> in the corresponding input tokens.</p> <h4 id="stable-rollout-in-diffusion-forcing">Stable Rollout in Diffusion Forcing</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/9.stable_rollout.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p><strong>Diffusion Forcing</strong> suggests to deceive models that generated clean tokens are little noisy, preventing models from believing generated tokens as GT. However, this approach is out of distribution (OOD) inference, and there is no rule of thumb for “little noisy”.</p> <h4 id="stable-rollout-another-option">Stable Rollout (Another Option)</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/10.stable_rollout.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p>To avoid OOD, one may suggest add little noise to generated tokens and tell models that the tokens are noisy. However, this approach may dilute details in generated tokens.</p> <h4 id="dynamic-noise-augmentation-dna">Dynamic Noise Augmentation (DNA)</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/11.DNA.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p><strong>Oasis</strong><d-cite key="decart2024oasis"></d-cite> suggests Dynamic Noise Augmentation (DNA) to mitigate error accumulation.</p> <ul> <li>For initial denoising steps, conditioning tokens (generated tokens) are moderately noised since models tend to generate low-frequency features during initial steps.</li> <li>For last denoising steps, noise levels of conditioning tokens gradually decreases.</li> </ul> <h3 id="long-term-context-preservation">Long Term Context Preservation</h3> <table> <tbody> <tr> <td><img src="/blog/post/20250117/12.video.gif" alt="" style="margin:auto; display:block;width:50%; height:auto;"></td> </tr> </tbody> </table> <p>Through above approaches, <strong>Oasis</strong> can autoregressively generate long videos without much quality degradation. However, models do not have long time horizon memory, leading to inconsistent videos. While there is no innovative breakthrough yet, I believe that video models with long-term memory is an important next step.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/diffusion.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Junoh-Kang/Junoh-Kang.github.io","data-repo-id":"R_kgDOJ2HI5w","data-category":"General","data-category-id":"DIC_kwDOJ2HI584CZLhE","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Junoh Kang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: January 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>