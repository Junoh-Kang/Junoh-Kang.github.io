<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="overview">Overview</h2> <p><strong>World simulators</strong> are explorable and interactive systems or models that can mimic real world. Advanced video generation models can function as world simulators, and to achieve it, they should have <strong>low latency</strong> for input actions, and capable of <strong>long sequence generation</strong>. Long sequence generation includes <strong><u>capability of long generation itself</u></strong>, <strong><u>preventing error accumulation</u></strong>, and <strong><u>long term context preservation</u></strong>. This post mainly focuses on how related project <strong>Oasis: A Universe in a transformer</strong><d-cite key="decart2024oasis"></d-cite> deals with <strong>long sequence generation</strong>.</p> <hr> <h2 id="conventional-long-video-generation-are-inappropriate-for-world-simulator">Conventional Long Video Generation are Inappropriate for World Simulator!</h2> <h3 id="video-diffusion-models-vdms">Video Diffusion Models (VDMs)</h3> <table> <thead> <tr> <th style="text-align: left"><img src="/blog/post/20250117/1.vdm.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Training and sampling of video diffusion models. Darker tokens has higher noise levels.</td> </tr> </tbody> </table> <p><strong><u>Videos are sequential data</u></strong>. However, video diffusion models are trained and inferenced to denoise tokens of same noise levels, interpreting each video clip as a single object. This section reviews approaches to generate long videos using aforementioned VDMs.</p> <h3 id="chunked-autoregressive-methods">Chunked Autoregressive Methods</h3> <table> <tbody> <tr> <td><img src="/blog/post/20250117/2.chunked.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <ul> <li>Small \(k\) (<em>e.g.</em> \(k=1\)) results in high latency for each action since \(f-k\) frames are output for each action. Also it tends to lose contexts.</li> <li>Large \(k\) (<em>e.g.</em> \(k=f-1\)) results in ineifficient training and inference since models learn only \(f-k\) tokens, while models calculate for \(f\) tokens.</li> <li>Chunked autoregressive methods suffers from <strong><u>quality degradation originated from error accumulation</u></strong>.</li> </ul> <table> <tbody> <tr> <td><img src="/blog/post/20250117/3.chunked_erroraccumulate.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <h3 id="hierarchical-methods-multi-stage-generation">Hierarchical Methods (Multi-stage Generation)</h3> <table> <tbody> <tr> <td><img src="/blog/post/20250117/4.hierarchy.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <ul> <li>It does not fit to interactive generation since the end of the video is already determined.</li> </ul> <p><strong>Conventional approaches are not appropriate for wolrd simulator!</strong></p> <table> <tbody> <tr> <td><img src="/blog/post/20250117/5.convention.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <hr> <h2 id="long-sequence-generation-in-oasis">Long Sequence Generation in Oasis</h2> <h3 id="capability-of-long-sequence-generation">Capability of Long Sequence Generation</h3> <p><strong>Oasis</strong><d-cite key="decart2024oasis"></d-cite> follows <strong>Diffusion Forcing</strong><d-cite key="chen2024diffusionforcing"></d-cite> to train models for long video generation. <strong>Diffusion Forcing</strong> inherits advantages of Teacher Forcing and Diffusion Models: <strong><u>flexible time horizon</u></strong> from Teacher Forcing, <strong><u>guidance at sampling</u></strong> from Diffusion Models.</p> <p><strong>Diffusion Forcing</strong> trains models to denoise <strong><u>tokens with independent noise levels</u></strong>, and sampling noise schedules are carefully chosen depending on the purpose. The training offers cheaper training than next-token prediction in video domain, and the complexity added by independent noise level is not excessive since the complexity is only in temporal dimension.</p> <table> <thead> <tr> <th style="text-align: center"><img src="/blog/post/20250117/6.df_train.png" alt="" style="margin:auto; display:block;width:50%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Training in Diffusion Forcing</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: center"><img src="/blog/post/20250117/7.df_sample.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Sampling in Diffusion Forcing</td> </tr> </tbody> </table> <h3 id="preventing-error-accumulation">Preventing Error Accumulation</h3> <h4 id="the-reason-of-error-accumulation">The reason of Error Accumulation</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/8.vanilla.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p><strong>Oasis</strong><d-cite key="decart2024oasis"></d-cite> and <strong>Diffusion Forcing</strong><d-cite key="chen2024diffusionforcing"></d-cite> hypothesize that the error accumulation stems from the model erroneously treating generated noisy frames as grount truth (GT), despite their inherent inaccuracies. They interpret <strong>input noise levels to the models as inversely proportional to the confidence</strong> in the corresponding input tokens.</p> <h4 id="stable-rollout-in-diffusion-forcing">Stable Rollout in Diffusion Forcing</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/9.stable_rollout.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p><strong>Diffusion Forcing</strong> suggests to deceive models that generated clean tokens are little noisy, preventing models from believing generated tokens as GT. However, this approach is out of distribution (OOD) inference, and there is no rule of thumb for “little noisy”.</p> <h4 id="stable-rollout-another-option">Stable Rollout (Another Option)</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/10.stable_rollout.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p>To avoid OOD, one may suggest add little noise to generated tokens and tell models that the tokens are noisy. However, this approach may dilute details in generated tokens.</p> <h4 id="dynamic-noise-augmentation-dna">Dynamic Noise Augmentation (DNA)</h4> <table> <tbody> <tr> <td><img src="/blog/post/20250117/11.DNA.png" alt="" style="margin:auto; display:block;width:90%; height:auto;"></td> </tr> </tbody> </table> <p><strong>Oasis</strong><d-cite key="decart2024oasis"></d-cite> suggests Dynamic Noise Augmentation (DNA) to mitigate error accumulation.</p> <ul> <li>For initial denoising steps, conditioning tokens (generated tokens) are moderately noised since models tend to generate low-frequency features during initial steps.</li> <li>For last denoising steps, noise levels of conditioning tokens gradually decreases.</li> </ul> <h3 id="long-term-context-preservation">Long Term Context Preservation</h3> <table> <tbody> <tr> <td><img src="/blog/post/20250117/12.video.gif" alt="" style="margin:auto; display:block;width:50%; height:auto;"></td> </tr> </tbody> </table> <p>Through above approaches, <strong>Oasis</strong> can autoregressively generate long videos without much quality degradation. However, models do not have long time horizon memory, leading to inconsistent videos. While there is no innovative breakthrough yet, I believe that video models with long-term memory is an important next step.</p> </body></html>