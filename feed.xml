<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://junoh-kang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://junoh-kang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-02T06:39:17+00:00</updated><id>https://junoh-kang.github.io/feed.xml</id><subtitle></subtitle><entry><title type="html">Elucidating the Design Space of Diffusion-Based Generative Models</title><link href="https://junoh-kang.github.io/blog/2023/paper_review-edm/" rel="alternate" type="text/html" title="Elucidating the Design Space of Diffusion-Based Generative Models"/><published>2023-07-24T00:00:00+00:00</published><updated>2023-07-24T00:00:00+00:00</updated><id>https://junoh-kang.github.io/blog/2023/paper_review:edm</id><content type="html" xml:base="https://junoh-kang.github.io/blog/2023/paper_review-edm/"><![CDATA[<h4 id="contribution-summary">Contribution Summary</h4> <ul> <li>The paper entangles independent components in diffusion model</li> <li>Higher-order Runge-Kutta method</li> <li>Third contribution : training of the score-modelig neural network, noise level, non-leaking augmentation</li> </ul> <h4 id="disentangle-components-of-diffusion-models">Disentangle components of diffusion models</h4> <h4 id="training-denoising-network"><strong>Training Denoising Network</strong></h4> <h4 id="forward-process-formulation">Forward process formulation</h4> <p><a href="https://arxiv.org/abs/2011.13456">Song et al.</a> define the forward SDE/ODE as \(\begin{align} dx = f(x,t)dt + g(t)dw. \nonumber \end{align}\) However, instead of defining $f(\cdot,\cdot)$ and $g(\cdot)$, EDM formulates forward SDE/ODE to follow the marginal distribution, \(p_{t}(x(t)|x(0)) = \mathcal{N}(x(t);s(t)x_0, s^2(t) \sigma^2(t) \mathrm{I})\). Then the corresponding ODE is \(\begin{align} dx = [\dot{s}(t)x/s(t) - s^2(t)\dot{\sigma}(t)\sigma(t) \nabla_x \log p(x/s(t); \sigma(t))]dt, \nonumber \end{align}\) where \(p(x;\sigma)=p_{data} * \mathcal{N}(0, \sigma^2(t)\mathrm{I})\) and \(p_t(x)=s^{-d}(t) p(x/s(t); \sigma(t))\).</p> <h4 id="parametrization">Parametrization</h4> <h4 id="preconditioning">Preconditioning</h4> <p>Previous Method : \(D_\theta(x;\sigma) = x - \sigma F_\theta(\cdot)\) <br/> EDM : \(D_\theta(x;\sigma) = c_{skip}(\sigma) x + c_{out}(\sigma) F_\theta(c_{in}(\sigma)x;c_{noise}(\sigma))\)</p> <h4 id="sampling"><strong>Sampling</strong></h4> <h4 id="ode-solver">ODE Solver</h4> <p>Euler’s method -&gt; Heun’s second order method Write Algorithm later</p> <h4 id="discretization">Discretization</h4> <p>Step size should decrease monotonically with decreasing \(\sigma\). \begin{align} \sigma_{i&lt;N} = ({\sigma_{max}}^{\frac{1}{\rho}} + \frac{i}{N-1}({\sigma_{min}}^\frac{1}{\rho} - {\sigma_{max}}^\frac{1}{\rho}))^{\rho}, \sigma_{N} = 0 \nonumber \end{align} Larger \(\rho\) results in shorter steps near \(\sigma_{min}\) and larger steps near \(\sigma_{max}\). EDM uses \(\rho=7\), while \(\rho=3\) nearly eualizes the truncation error at each timestep.</p> <h4 id="results"><strong>Results</strong></h4> <table> <thead> <tr> <th>제목 셀1</th> <th>제목 셀2</th> <th>제목 셀3</th> <th>제목 셀4</th> </tr> </thead> <tbody> <tr> <td>내용 1</td> <td>내용 2</td> <td>내용 3</td> <td>내용 4</td> </tr> <tr> <td>내용 5</td> <td>내용 6</td> <td>내용 7</td> <td>내용 8</td> </tr> <tr> <td>내용 9</td> <td>내용 10</td> <td>내용 11</td> <td>내용 12</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="paper-review"/><summary type="html"><![CDATA[Contribution Summary The paper entangles independent components in diffusion model Higher-order Runge-Kutta method Third contribution : training of the score-modelig neural network, noise level, non-leaking augmentation]]></summary></entry></feed>